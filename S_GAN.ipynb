{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S-GAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c36ff3a59aaa492a8482c8ce4fce32fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a62673f22ba64d9d899090ad422a0ddf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e59bcbd641a749c2b7d863fcde68ad94",
              "IPY_MODEL_f1a29ad285814bc1b6b50555575c775a"
            ]
          }
        },
        "a62673f22ba64d9d899090ad422a0ddf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e59bcbd641a749c2b7d863fcde68ad94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7462185063e44dbd92f71a435cbdf483",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "",
            "max": 200,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 80,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4be5496987364f78ac182737b78f3edb"
          }
        },
        "f1a29ad285814bc1b6b50555575c775a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dddb0e2081e0484d8b6ce1d8ef9b1578",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 40% 80/200 [00:51&lt;01:14,  1.60it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_00383a57eaf245909b017def2dc4f52a"
          }
        },
        "7462185063e44dbd92f71a435cbdf483": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4be5496987364f78ac182737b78f3edb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dddb0e2081e0484d8b6ce1d8ef9b1578": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "00383a57eaf245909b017def2dc4f52a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDXck-2aGkEz",
        "colab_type": "code",
        "outputId": "a139ea07-234b-47cb-a914-c2a18bee5bca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install torch # framework\n",
        "!pip install --upgrade reedsolo"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already up-to-date: reedsolo in /usr/local/lib/python3.6/dist-packages (1.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ7QF4YbN1Pl",
        "colab_type": "code",
        "outputId": "557f6e36-a62f-4611-8c06-5f080d4620e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') \n",
        "%cd /content/drive/My\\ Drive/"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCY2rWEiOqDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn.functional import binary_cross_entropy_with_logits, mse_loss\n",
        "from torchvision import datasets, transforms\n",
        "from IPython.display import clear_output\n",
        "import torchvision\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm_notebook\n",
        "import torch\n",
        "import os\n",
        "import gc\n",
        "from PIL import ImageFile, Image\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_dX58S6hOoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = 'div2k'\n",
        "epochs = 32\n",
        "data_depth = 2\n",
        "hidden_size = 32\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "LOAD_MODEL=False\n",
        "PATH='/content/drive/My Drive/results/model/EN_DE_-0.009_2020-03-04_11:21:33.dat'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S0bTwfpK3YG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from math import exp\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import zlib\n",
        "from math import exp\n",
        "\n",
        "import torch\n",
        "from reedsolo import RSCodec\n",
        "from torch.nn.functional import conv2d\n",
        "\n",
        "rs = RSCodec(250)\n",
        "\n",
        "\n",
        "def text_to_bits(text):\n",
        "    \"\"\"Convert text to a list of ints in {0, 1}\"\"\"\n",
        "    return bytearray_to_bits(text_to_bytearray(text))\n",
        "\n",
        "\n",
        "def bits_to_text(bits):\n",
        "    \"\"\"Convert a list of ints in {0, 1} to text\"\"\"\n",
        "    return bytearray_to_text(bits_to_bytearray(bits))\n",
        "\n",
        "\n",
        "def bytearray_to_bits(x):\n",
        "    \"\"\"Convert bytearray to a list of bits\"\"\"\n",
        "    result = []\n",
        "    for i in x:\n",
        "        bits = bin(i)[2:]\n",
        "        bits = '00000000'[len(bits):] + bits\n",
        "        result.extend([int(b) for b in bits])\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def bits_to_bytearray(bits):\n",
        "    \"\"\"Convert a list of bits to a bytearray\"\"\"\n",
        "    ints = []\n",
        "    for b in range(len(bits) // 8):\n",
        "        byte = bits[b * 8:(b + 1) * 8]\n",
        "        ints.append(int(''.join([str(bit) for bit in byte]), 2))\n",
        "\n",
        "    return bytearray(ints)\n",
        "\n",
        "\n",
        "def text_to_bytearray(text):\n",
        "    \"\"\"Compress and add error correction\"\"\"\n",
        "    assert isinstance(text, str), \"expected a string\"\n",
        "    x = zlib.compress(text.encode(\"utf-8\"))\n",
        "    x = rs.encode(bytearray(x))\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def bytearray_to_text(x):\n",
        "    \"\"\"Apply error correction and decompress\"\"\"\n",
        "    try:\n",
        "        text = rs.decode(x)\n",
        "        text = zlib.decompress(text)\n",
        "        return text.decode(\"utf-8\")\n",
        "    except BaseException:\n",
        "        return False\n",
        "\n",
        "def gaussian(window_size, sigma):\n",
        "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
        "    return gauss/gauss.sum()\n",
        "\n",
        "def create_window(window_size, channel):\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
        "    return window\n",
        "\n",
        "def _ssim(img1, img2, window, window_size, channel, size_average = True):\n",
        "    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n",
        "    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n",
        "\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1*mu2\n",
        "\n",
        "    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n",
        "    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n",
        "    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n",
        "\n",
        "    C1 = 0.01**2\n",
        "    C2 = 0.03**2\n",
        "\n",
        "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
        "\n",
        "    if size_average:\n",
        "        return ssim_map.mean()\n",
        "    else:\n",
        "        return ssim_map.mean(1).mean(1).mean(1)\n",
        "\n",
        "class SSIM(torch.nn.Module):\n",
        "    def __init__(self, window_size = 11, size_average = True):\n",
        "        super(SSIM, self).__init__()\n",
        "        self.window_size = window_size\n",
        "        self.size_average = size_average\n",
        "        self.channel = 1\n",
        "        self.window = create_window(window_size, self.channel)\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        (_, channel, _, _) = img1.size()\n",
        "\n",
        "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
        "            window = self.window\n",
        "        else:\n",
        "            window = create_window(self.window_size, channel)\n",
        "            \n",
        "            if img1.is_cuda:\n",
        "                window = window.cuda(img1.get_device())\n",
        "            window = window.type_as(img1)\n",
        "            \n",
        "            self.window = window\n",
        "            self.channel = channel\n",
        "\n",
        "\n",
        "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
        "\n",
        "def ssim(img1, img2, window_size = 11, size_average = True):\n",
        "    (_, channel, _, _) = img1.size()\n",
        "    window = create_window(window_size, channel)\n",
        "    \n",
        "    if img1.is_cuda:\n",
        "        window = window.cuda(img1.get_device())\n",
        "    window = window.type_as(img1)\n",
        "    \n",
        "    return _ssim(img1, img2, window, window_size, channel, size_average)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO7Y-DuVKj38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy\n",
        "\n",
        "\n",
        "class BasicEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The BasicEncoder module takes an cover image and a data tensor and combines\n",
        "    them into a steganographic image.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def _conv2d(self, in_channels, out_channels):\n",
        "        return nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1\n",
        "        )\n",
        "\n",
        "    def _build_models(self):\n",
        "        self.conv1 = nn.Sequential(\n",
        "            self._conv2d(3, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size + self.data_depth, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size, 3),\n",
        "        )\n",
        "        return self.conv1, self.conv2, self.conv3, self.conv4\n",
        "\n",
        "    def __init__(self, data_depth, hidden_size):\n",
        "        super().__init__()\n",
        "        self.data_depth = data_depth\n",
        "        self.hidden_size = hidden_size\n",
        "        self._models = self._build_models()\n",
        "\n",
        "    def forward(self, image, data):\n",
        "        x = self._models[0](image)\n",
        "        x_1 = self._models[1](torch.cat([x] + [data], dim=1))\n",
        "        x_2 = self._models[2](x_1)\n",
        "        x_3 = self._models[3](x_2)\n",
        "        return x_3\n",
        "\n",
        "    def _name():\n",
        "      return \"BasicEncoder\"\n",
        "\n",
        "class ResidualEncoder(BasicEncoder):\n",
        "\n",
        "    def forward(self, image, data):\n",
        "        return image + super().forward(self, image, data)\n",
        "\n",
        "    def _name():\n",
        "      return \"ResidualEncoder\"\n",
        "\n",
        "class DenseEncoder(ResidualEncoder):\n",
        "\n",
        "    def _build_models(self):\n",
        "        self.conv1 = super()._models[0]\n",
        "        self.conv2 = super()._models[1]\n",
        "        self.conv3 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size * 2 +\n",
        "                         self.data_depth, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size * 3 + self.data_depth, 3)\n",
        "        )\n",
        "\n",
        "        return self.conv1, self.conv2, self.conv3, self.conv4\n",
        "\n",
        "    def forward(self, image, data):\n",
        "        x = self._models[0](image)\n",
        "        x_list = [x]\n",
        "        x_1 = self._models[1](torch.cat(x_list+[data], dim=1))\n",
        "        x_list.append(x_1)\n",
        "        x_2 = self._models[2](torch.cat(x_list+[data], dim=1))\n",
        "        x_list.append(x_2)\n",
        "        x_3 = self._models[3](torch.cat(x_list+[data], dim=1))\n",
        "        x_list.append(x_3)\n",
        "        return image + x_3\n",
        "\n",
        "    def _name():\n",
        "      return \"DenseEncoder\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPc3PmGwKoZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import Softmax\n",
        "\n",
        "\n",
        "class BasicDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The BasicDecoder module takes an steganographic image and attempts to decode\n",
        "    the embedded data tensor.\n",
        "\n",
        "    Input: (N, 3, H, W)\n",
        "    Output: (N, D, H, W)\n",
        "    \"\"\"\n",
        "\n",
        "    def _conv2d(self, in_channels, out_channels):\n",
        "        return nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1\n",
        "        )\n",
        "\n",
        "    def _build_models(self):\n",
        "        self.conv1 = nn.Sequential(\n",
        "            self._conv2d(3, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size, self.data_depth),\n",
        "            nn.Softmax(),\n",
        "        )\n",
        "\n",
        "        return self.conv1, self.conv2, self.conv3, self.conv4\n",
        "\n",
        "    def forward(self, image):\n",
        "        x = self._models[0](image)\n",
        "        x_1 = self._models[1](x)\n",
        "        x_2 = self._models[2](x_1)\n",
        "        x_3 = self._models[3](x_2)\n",
        "        return x_3\n",
        "\n",
        "    def __init__(self, data_depth, hidden_size):\n",
        "        super().__init__()\n",
        "        self.data_depth = data_depth\n",
        "        self.hidden_size = hidden_size\n",
        "        self._models = self._build_models()\n",
        "    \n",
        "    def _name():\n",
        "      return \"BasicDecoder\"\n",
        "\n",
        "\n",
        "class DenseDecoder(BasicDecoder):\n",
        "\n",
        "    def _build_models(self):\n",
        "        self.conv1 = super()._models[0]\n",
        "        self.conv2 = super()._models[1]\n",
        "        self.conv3 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size * 2, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size)\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size * 3, self.data_depth),\n",
        "            nn.Softmax(),\n",
        "        )\n",
        "\n",
        "        return self.conv1, self.conv2, self.conv3, self.conv4\n",
        "\n",
        "    def forward(self, image):\n",
        "        x = self._models[0](image)\n",
        "        x_list = [x]\n",
        "        x_1 = self._models[1](torch.cat(x_list, dim=1))\n",
        "        x_list.append(x_1)\n",
        "        x_2 = self._models[2](torch.cat(x_list, dim=1))\n",
        "        x_list.append(x_2)\n",
        "        x_3 = self._models[3](torch.cat(x_list, dim=1))\n",
        "        x_list.append(x_3)\n",
        "        return x_3\n",
        "\n",
        "    def _name():\n",
        "      return \"DenseDecoder\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he1SeqWRKtb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class BasicCritic(nn.Module):\n",
        "    \"\"\"\n",
        "    The BasicCritic module takes an image and predicts whether it is a cover\n",
        "    image or a steganographic image (N, 1).\n",
        "\n",
        "    Input: (N, 3, H, W)\n",
        "    Output: (N, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    def _conv2d(self, in_channels, out_channels):\n",
        "        return nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3\n",
        "        )\n",
        "\n",
        "    def _build_models(self):\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            self._conv2d(3, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )  \n",
        "        self.conv4 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size, 1)\n",
        "        )         \n",
        "\n",
        "        return self.conv1,self.conv2,self.conv3,self.conv4\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self._models = self._build_models()\n",
        "\n",
        "    def forward(self, image):\n",
        "        x = self._models[0](image)\n",
        "        x_1 = self._models[1](x)\n",
        "        x_2 = self._models[2](x_1)\n",
        "        x_3 = self._models[3](x_2)\n",
        "        return torch.mean(x_3.view(x_3.size(0), -1), dim=1)\n",
        "    \n",
        "    def _name():\n",
        "      return \"BasicCritic\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVLWLMetCrGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot(name, train_epoch, values, save):\n",
        "    clear_output(wait=True)\n",
        "    plt.close('all')\n",
        "    fig = plt.figure()\n",
        "    fig = plt.ion()\n",
        "    fig = plt.subplot(1, 1, 1)\n",
        "    fig = plt.title('epoch: %s -> %s: %s' % (train_epoch, name, values[-1]))\n",
        "    fig = plt.ylabel(name)\n",
        "    fig = plt.xlabel('validation_set')\n",
        "    fig = plt.plot(values)\n",
        "    fig = plt.grid()\n",
        "    get_fig = plt.gcf()\n",
        "    fig = plt.draw()  # draw the plot\n",
        "    fig = plt.pause(1)  # show it for 1 second\n",
        "    if save:\n",
        "        now = datetime.datetime.now()\n",
        "        get_fig.savefig('results/plots/%s_%.3f_%d_%s.png' %\n",
        "                        (name, train_epoch, values[-1], now.strftime(\"%Y-%m-%d_%H:%M:%S\")))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZCf5oX12gWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(encoder,decoder, cover, payload):\n",
        "  %matplotlib inline\n",
        "  generated = encoder.forward(cover, payload)\n",
        "  decoded = decoder.forward(generated)\n",
        "  decoder_loss = binary_cross_entropy_with_logits(decoded, payload)\n",
        "  decoder_acc = (decoded >= 0.0).eq(\n",
        "    payload >= 0.5).sum().float() / payload.numel() # .numel() calculate the number of element in a tensor\n",
        "  print(\"Decoder loss: %.3f\"% decoder_loss.item())\n",
        "  print(\"Decoder acc: %.3f\"% decoder_acc.item())\n",
        "  f, ax = plt.subplots(1, 2)\n",
        "  cover=np.transpose(np.squeeze(cover.cpu()), (1, 2, 0))\n",
        "  ax[0].imshow(cover)\n",
        "  ax[0].axis('off')\n",
        "  generated=np.transpose(np.squeeze((generated.cpu()).detach().numpy()), (1, 2, 0))\n",
        "  ax[1].imshow(generated)\n",
        "  ax[1].axis('off')\n",
        "  now = datetime.datetime.now()\n",
        "  print(\"payload :\")\n",
        "  print(payload)\n",
        "  print(\"decoded :\")\n",
        "  print(decoded)\n",
        "  #get_fig.savefig('results/samples/%s_%.3f_%d_%s.png' %\n",
        "              #(name, train_epoch, values[-1], now.strftime(\"%Y-%m-%d_%H:%M:%S\")))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHGbk-t3imrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(encoder,decoder,critic,en_de_optimizer,cr_optimizer,metrics,ep):\n",
        "    now = datetime.datetime.now()\n",
        "    cover_score = metrics['val.cover_score'][-1]\n",
        "    name = \"%s_%s_%+.3f_%s.dat\" % (encoder._name(),decoder._name(),cover_score,\n",
        "                                   now.strftime(\"%Y-%m-%d_%H:%M:%S\"))\n",
        "    fname = os.path.join('.', 'results/model', name)\n",
        "    states = {\n",
        "            'state_dict_critic': critic.state_dict(),\n",
        "            'state_dict_encoder': encoder.state_dict(),\n",
        "            'state_dict_decoder': decoder.state_dict(),\n",
        "            'en_de_optimizer': en_de_optimizer.state_dict(),\n",
        "            'cr_optimizer': cr_optimizer.state_dict(),\n",
        "            'metrics': metrics,\n",
        "            'train_epoch': ep,\n",
        "            'date': now.strftime(\"%Y-%m-%d_%H:%M:%S\"),\n",
        "    }\n",
        "    torch.save(states, fname)\n",
        "    plot('encoder_mse', ep, metrics['val.encoder_mse'], True)\n",
        "    plot('decoder_loss', ep, metrics['val.decoder_loss'], True)\n",
        "    plot('decoder_acc', ep, metrics['val.decoder_acc'], True)\n",
        "    plot('cover_score', ep, metrics['val.cover_score'], True)\n",
        "    plot('generated_score', ep, metrics['val.generated_score'], True)\n",
        "    plot('ssim', ep, metrics['val.ssim'], True)\n",
        "    plot('psnr', ep, metrics['val.psnr'], True)\n",
        "    plot('bpp', ep, metrics['val.bpp'], True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMqZ9W6Ug6dR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_gan(encoder,decoder,critic,en_de_optimizer,cr_optimizer,metrics,train_loader,valid_loader):\n",
        "      for ep in range(epochs):\n",
        "        for cover, _ in tqdm_notebook(train_loader):\n",
        "            gc.collect()\n",
        "            cover = cover.to(device)\n",
        "            N, _, H, W = cover.size()\n",
        "            # sampled from the discrete uniform distribution over 0 to 2\n",
        "            payload = torch.zeros((N, data_depth, H, W),\n",
        "                                  device=device).random_(0, 2)\n",
        "            generated = encoder.forward(cover, payload)\n",
        "            cover_score = torch.mean(critic.forward(cover))\n",
        "            generated_score = torch.mean(critic.forward(generated))\n",
        "\n",
        "            cr_optimizer.zero_grad()\n",
        "            (cover_score - generated_score).backward(retain_graph=False)\n",
        "            cr_optimizer.step()\n",
        "\n",
        "            for p in critic.parameters():\n",
        "                p.data.clamp_(-0.1, 0.1)\n",
        "            metrics['train.cover_score'].append(cover_score.item())\n",
        "            metrics['train.generated_score'].append(generated_score.item())\n",
        "\n",
        "        for cover, _ in tqdm_notebook(train_loader):\n",
        "            gc.collect()\n",
        "            cover = cover.to(device)\n",
        "            N, _, H, W = cover.size()\n",
        "            # sampled from the discrete uniform distribution over 0 to 2\n",
        "            payload = torch.zeros((N, data_depth, H, W),\n",
        "                                  device=device).random_(0, 2)\n",
        "            generated = encoder.forward(cover, payload)\n",
        "            decoded = decoder.forward(generated)\n",
        "\n",
        "            encoder_mse = mse_loss(generated, cover)\n",
        "            decoder_loss = binary_cross_entropy_with_logits(decoded, payload)\n",
        "            decoder_acc = (decoded >= 0.0).eq(\n",
        "                payload >= 0.5).sum().float() / payload.numel()\n",
        "            generated_score = torch.mean(critic.forward(generated))\n",
        "\n",
        "            en_de_optimizer.zero_grad()\n",
        "            (encoder_mse + decoder_loss +\n",
        "             generated_score).backward()  # Why 100?\n",
        "            en_de_optimizer.step()\n",
        "\n",
        "            metrics['train.encoder_mse'].append(encoder_mse.item())\n",
        "            metrics['train.decoder_loss'].append(decoder_loss.item())\n",
        "            metrics['train.decoder_acc'].append(decoder_acc.item())\n",
        "\n",
        "        for cover, _ in tqdm_notebook(valid_loader):\n",
        "            gc.collect()\n",
        "            cover = cover.to(device)\n",
        "            N, _, H, W = cover.size()\n",
        "            # sampled from the discrete uniform distribution over 0 to 2\n",
        "            payload = torch.zeros((N, data_depth, H, W),\n",
        "                                  device=device).random_(0, 2)\n",
        "            generated = encoder.forward(cover, payload)\n",
        "            decoded = decoder.forward(generated)\n",
        "\n",
        "            encoder_mse = mse_loss(generated, cover)\n",
        "            decoder_loss = binary_cross_entropy_with_logits(decoded, payload)\n",
        "            decoder_acc = (decoded >= 0.0).eq(\n",
        "                payload >= 0.5).sum().float() / payload.numel()\n",
        "            generated_score = torch.mean(critic.forward(generated))\n",
        "            cover_score = torch.mean(critic.forward(cover))\n",
        "\n",
        "            metrics['val.encoder_mse'].append(encoder_mse.item())\n",
        "            metrics['val.decoder_loss'].append(decoder_loss.item())\n",
        "            metrics['val.decoder_acc'].append(decoder_acc.item())\n",
        "            metrics['val.cover_score'].append(cover_score.item())\n",
        "            metrics['val.generated_score'].append(generated_score.item())\n",
        "            metrics['val.ssim'].append(\n",
        "                ssim(cover, generated).item())\n",
        "            metrics['val.psnr'].append(\n",
        "                10 * torch.log10(4 / encoder_mse).item())\n",
        "            metrics['val.bpp'].append(\n",
        "                data_depth * (2 * decoder_acc.item() - 1))\n",
        "        save_model(encoder,decoder,critic,en_de_optimizer,cr_optimizer,metrics,ep)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF4iLYgOOm0t",
        "colab_type": "code",
        "outputId": "b8a27d11-1e43-444c-bebc-e544a2c540f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "c36ff3a59aaa492a8482c8ce4fce32fa",
            "a62673f22ba64d9d899090ad422a0ddf",
            "e59bcbd641a749c2b7d863fcde68ad94",
            "f1a29ad285814bc1b6b50555575c775a",
            "7462185063e44dbd92f71a435cbdf483",
            "4be5496987364f78ac182737b78f3edb",
            "dddb0e2081e0484d8b6ce1d8ef9b1578",
            "00383a57eaf245909b017def2dc4f52a"
          ]
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  for func in [\n",
        "            lambda: os.mkdir(os.path.join('.', 'results')),\n",
        "            lambda: os.mkdir(os.path.join('.', 'results/model')),\n",
        "            lambda: os.mkdir(os.path.join('.', 'results/plots'))]:  # create directories\n",
        "    try:\n",
        "      func()\n",
        "    except Exception as error:\n",
        "      print(error)\n",
        "      continue\n",
        "\n",
        "  METRIC_FIELDS = [\n",
        "        'val.encoder_mse',\n",
        "        'val.decoder_loss',\n",
        "        'val.decoder_acc',\n",
        "        'val.cover_score',\n",
        "        'val.generated_score',\n",
        "        'val.ssim',\n",
        "        'val.psnr',\n",
        "        'val.bpp',\n",
        "        'train.encoder_mse',\n",
        "        'train.decoder_loss',\n",
        "        'train.decoder_acc',\n",
        "        'train.cover_score',\n",
        "        'train.generated_score',\n",
        "  ]\n",
        "\n",
        "  mu = [.5, .5, .5]\n",
        "  sigma = [.5, .5, .5]\n",
        "\n",
        "  transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                    transforms.RandomCrop(\n",
        "                                        360, pad_if_needed=True),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize(mu, sigma)])\n",
        "\n",
        "  train_set = datasets.ImageFolder(os.path.join(\n",
        "        data_dir, \"train/\"), transform=transform)\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "        train_set, batch_size=4, shuffle=True)\n",
        "\n",
        "  valid_set = datasets.ImageFolder(os.path.join(\n",
        "        data_dir, \"val/\"), transform=transform)\n",
        "  valid_loader = torch.utils.data.DataLoader(\n",
        "        valid_set, batch_size=4, shuffle=False)\n",
        "\n",
        "  encoder = BasicEncoder(data_depth, hidden_size).to(device)\n",
        "  decoder = BasicDecoder(data_depth, hidden_size).to(device)\n",
        "  critic = BasicCritic(hidden_size).to(device)\n",
        "  cr_optimizer = Adam(critic.parameters(), lr=1e-4)\n",
        "  en_de_optimizer = Adam(list(decoder.parameters()) +\n",
        "                           list(encoder.parameters()), lr=1e-4)\n",
        "  metrics = {field: list() for field in METRIC_FIELDS}\n",
        "\n",
        "  if LOAD_MODEL: \n",
        "    if torch.cuda.is_available():\n",
        "      checkpoint = torch.load(PATH)\n",
        "    else:\n",
        "      checkpoint = torch.load(PATH, map_location=lambda storage, loc: storage)\n",
        "              \n",
        "    critic.load_state_dict(checkpoint['state_dict_critic'])\n",
        "    encoder.load_state_dict(checkpoint['state_dict_encoder'])\n",
        "    decoder.load_state_dict(checkpoint['state_dict_decoder'])\n",
        "    en_de_optimizer.load_state_dict(checkpoint['en_de_optimizer'])\n",
        "    cr_optimizer.load_state_dict(checkpoint['cr_optimizer'])\n",
        "    metrics=checkpoint['metrics']\n",
        "    train_epoch=checkpoint['train_epoch']\n",
        "    date=checkpoint['date']\n",
        "    critic.train()\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    print('GAN loaded')\n",
        "    print(critic)\n",
        "    print(encoder)\n",
        "    print(decoder)\n",
        "    print(en_de_optimizer)\n",
        "    print(cr_optimizer)\n",
        "    print(date)\n",
        "  else:\n",
        "    fit_gan(encoder,decoder,critic,en_de_optimizer,cr_optimizer,metrics,train_loader,valid_loader)\n",
        "                "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 17] File exists: './results'\n",
            "[Errno 17] File exists: './results/model'\n",
            "[Errno 17] File exists: './results/plots'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c36ff3a59aaa492a8482c8ce4fce32fa",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFFfYzJVK-I6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cover, _ = next(iter(valid_set))\n",
        "_, H, W = cover.size()\n",
        "cover = cover[None].to(device)\n",
        "payload = torch.zeros((1, data_depth, H, W),device=device).random_(0, 2)\n",
        "test(encoder,decoder,cover,payload)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D05o0AtBONk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}