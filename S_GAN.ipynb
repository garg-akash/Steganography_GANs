{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S-GAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6bea0c9de09640e3993372d36775e552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_67baca1aec9f452e8f3e4ff08896a4f4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_787da502c8df4c78bd7b804e700671f0",
              "IPY_MODEL_4cb603612a52484cac4af3803ef18b46"
            ]
          }
        },
        "67baca1aec9f452e8f3e4ff08896a4f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "787da502c8df4c78bd7b804e700671f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_08fe95a8994a48cbb086f036f37961d1",
            "_dom_classes": [],
            "description": "  1%",
            "_model_name": "IntProgressModel",
            "bar_style": "",
            "max": 223,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b3a441bc8a6b422085735ec55507f283"
          }
        },
        "4cb603612a52484cac4af3803ef18b46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_82540accf51f4368adf429f87e8ab6e2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3/223 [00:12&lt;14:50,  4.05s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_89f6cf8573844db59a71bbf53a7befa6"
          }
        },
        "08fe95a8994a48cbb086f036f37961d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b3a441bc8a6b422085735ec55507f283": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "82540accf51f4368adf429f87e8ab6e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "89f6cf8573844db59a71bbf53a7befa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDXck-2aGkEz",
        "colab_type": "code",
        "outputId": "3d975313-11b6-4b25-c2a4-664a1ce66575",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "!pip install torch # framework\n",
        "!pip install --upgrade reedsolo\n",
        "!python --version\n",
        "!pip install librosa\n",
        "!pip install torchvision"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Collecting reedsolo\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/f3/c5b6d252e70334d8613caf7b663454565451cae9f3da424e0be2ae7024e0/reedsolo-1.5.1.tar.gz (270kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276kB 4.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: reedsolo\n",
            "  Building wheel for reedsolo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for reedsolo: filename=reedsolo-1.5.1-cp36-cp36m-linux_x86_64.whl size=710233 sha256=b55d1add8d72c2970a8b83a4759a2d06de8cf27bd7e2e0b59be7b17bc98213c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/de/37/88251b06c35aafa0463c054639defdcecfc10b1c5b5c06fbe9\n",
            "Successfully built reedsolo\n",
            "Installing collected packages: reedsolo\n",
            "Successfully installed reedsolo-1.5.1\n",
            "Python 3.6.9\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (0.6.3)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.4.1)\n",
            "Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.47.0)\n",
            "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.18.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (2.1.8)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.12.0)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.14.1)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: llvmlite>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (46.0.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.2)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ7QF4YbN1Pl",
        "colab_type": "code",
        "outputId": "ffec10f7-1bac-4f4c-df23-e6c4365d04cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') \n",
        "%cd /content/drive/My\\ Drive/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCY2rWEiOqDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display \n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn.functional import binary_cross_entropy_with_logits, mse_loss\n",
        "from torchvision import datasets, transforms\n",
        "from IPython.display import clear_output\n",
        "import torchvision\n",
        "from torchvision.datasets.vision import VisionDataset\n",
        "from torch.optim import Adam\n",
        "from tqdm import notebook\n",
        "import torch\n",
        "import os\n",
        "import os.path\n",
        "import gc\n",
        "import sys\n",
        "from PIL import ImageFile, Image\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_dX58S6hOoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 4\n",
        "data_depth = 4\n",
        "hidden_size = 32\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "LOAD_MODEL=False\n",
        "PATH='/content/drive/My Drive/results/model/DenseEncoder_DenseDecoder_-0.001_2020-03-08_15:45:59.dat'\n",
        "AUD_EXTENSIONS = ('.flac', '.wav', '.mp3', '.mp4')\n",
        "audio = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S0bTwfpK3YG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from math import exp\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import zlib\n",
        "from math import exp\n",
        "\n",
        "import torch\n",
        "from reedsolo import RSCodec\n",
        "from torch.nn.functional import conv2d\n",
        "\n",
        "rs = RSCodec(250)\n",
        "\n",
        "\n",
        "def text_to_bits(text):\n",
        "    \"\"\"Convert text to a list of ints in {0, 1}\"\"\"\n",
        "    return bytearray_to_bits(text_to_bytearray(text))\n",
        "\n",
        "\n",
        "def bits_to_text(bits):\n",
        "    \"\"\"Convert a list of ints in {0, 1} to text\"\"\"\n",
        "    return bytearray_to_text(bits_to_bytearray(bits))\n",
        "\n",
        "\n",
        "def bytearray_to_bits(x):\n",
        "    \"\"\"Convert bytearray to a list of bits\"\"\"\n",
        "    result = []\n",
        "    for i in x:\n",
        "        bits = bin(i)[2:]\n",
        "        bits = '00000000'[len(bits):] + bits\n",
        "        result.extend([int(b) for b in bits])\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def bits_to_bytearray(bits):\n",
        "    \"\"\"Convert a list of bits to a bytearray\"\"\"\n",
        "    ints = []\n",
        "    for b in range(len(bits) // 8):\n",
        "        byte = bits[b * 8:(b + 1) * 8]\n",
        "        ints.append(int(''.join([str(bit) for bit in byte]), 2))\n",
        "\n",
        "    return bytearray(ints)\n",
        "\n",
        "\n",
        "def text_to_bytearray(text):\n",
        "    \"\"\"Compress and add error correction\"\"\"\n",
        "    assert isinstance(text, str), \"expected a string\"\n",
        "    x = zlib.compress(text.encode(\"utf-8\"))\n",
        "    x = rs.encode(bytearray(x))\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def bytearray_to_text(x):\n",
        "    \"\"\"Apply error correction and decompress\"\"\"\n",
        "    try:\n",
        "        text = rs.decode(x)\n",
        "        text = zlib.decompress(text)\n",
        "        return text.decode(\"utf-8\")\n",
        "    except BaseException:\n",
        "        return False\n",
        "\n",
        "def gaussian(window_size, sigma):\n",
        "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
        "    return gauss/gauss.sum()\n",
        "\n",
        "def create_window(window_size, channel):\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
        "    return window\n",
        "\n",
        "def _ssim(img1, img2, window, window_size, channel, size_average = True):\n",
        "    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n",
        "    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n",
        "\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1*mu2\n",
        "\n",
        "    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n",
        "    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n",
        "    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n",
        "\n",
        "    C1 = 0.01**2\n",
        "    C2 = 0.03**2\n",
        "\n",
        "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
        "\n",
        "    if size_average:\n",
        "        return ssim_map.mean()\n",
        "    else:\n",
        "        return ssim_map.mean(1).mean(1).mean(1)\n",
        "\n",
        "class SSIM(torch.nn.Module):\n",
        "    def __init__(self, window_size = 11, size_average = True):\n",
        "        super(SSIM, self).__init__()\n",
        "        self.window_size = window_size\n",
        "        self.size_average = size_average\n",
        "        self.channel = 1\n",
        "        self.window = create_window(window_size, self.channel)\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        (_, channel, _, _) = img1.size()\n",
        "\n",
        "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
        "            window = self.window\n",
        "        else:\n",
        "            window = create_window(self.window_size, channel)\n",
        "            \n",
        "            if img1.is_cuda:\n",
        "                window = window.cuda(img1.get_device())\n",
        "            window = window.type_as(img1)\n",
        "            \n",
        "            self.window = window\n",
        "            self.channel = channel\n",
        "\n",
        "\n",
        "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
        "\n",
        "def ssim(img1, img2, window_size = 11, size_average = True):\n",
        "    (_, channel, _, _) = img1.size()\n",
        "    window = create_window(window_size, channel)\n",
        "    \n",
        "    if img1.is_cuda:\n",
        "        window = window.cuda(img1.get_device())\n",
        "    window = window.type_as(img1)\n",
        "    \n",
        "    return _ssim(img1, img2, window, window_size, channel, size_average)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO7Y-DuVKj38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy\n",
        "\n",
        "\n",
        "class BasicEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The BasicEncoder module takes an cover image and a data tensor and combines\n",
        "    them into a steganographic image.\n",
        "\n",
        "    \"\"\"\n",
        "    def _name(self):\n",
        "      return \"BasicEncoder\"\n",
        "\n",
        "    def _conv2d(self, in_channels, out_channels):\n",
        "        return nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1\n",
        "        )\n",
        "\n",
        "    def _build_models(self):\n",
        "        self.conv1 = nn.Sequential(\n",
        "            self._conv2d(3, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size + self.data_depth, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size, 3),\n",
        "        )\n",
        "        return self.conv1, self.conv2, self.conv3, self.conv4\n",
        "\n",
        "    def __init__(self, data_depth, hidden_size):\n",
        "        super().__init__()\n",
        "        self.data_depth = data_depth\n",
        "        self.hidden_size = hidden_size\n",
        "        self._models = self._build_models()\n",
        "        self.name = self._name()\n",
        "\n",
        "    def forward(self, image, data):\n",
        "        x = self._models[0](image)\n",
        "        x_1 = self._models[1](torch.cat([x] + [data], dim=1))\n",
        "        x_2 = self._models[2](x_1)\n",
        "        x_3 = self._models[3](x_2)\n",
        "        return x_3\n",
        "\n",
        "\n",
        "class ResidualEncoder(BasicEncoder):\n",
        "    def _name(self):\n",
        "      return \"ResidualEncoder\"\n",
        "\n",
        "    def forward(self, image, data):\n",
        "        return image + super().forward(self, image, data)\n",
        "\n",
        "\n",
        "class DenseEncoder(BasicEncoder):\n",
        "    def _name(self):\n",
        "      return \"DenseEncoder\"\n",
        "\n",
        "    def _build_models(self):\n",
        "        self.conv1 = super()._build_models()[0]\n",
        "        self.conv2 = super()._build_models()[1]\n",
        "        self.conv3 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size * 2 +\n",
        "                         self.data_depth, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size * 3 + self.data_depth, 3)\n",
        "        )\n",
        "\n",
        "        return self.conv1, self.conv2, self.conv3, self.conv4\n",
        "\n",
        "    def forward(self, image, data):\n",
        "        x = self._models[0](image)\n",
        "        x_list = [x]\n",
        "        x_1 = self._models[1](torch.cat(x_list+[data], dim=1))\n",
        "        x_list.append(x_1)\n",
        "        x_2 = self._models[2](torch.cat(x_list+[data], dim=1))\n",
        "        x_list.append(x_2)\n",
        "        x_3 = self._models[3](torch.cat(x_list+[data], dim=1))\n",
        "        x_list.append(x_3)\n",
        "        return image + x_3\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPc3PmGwKoZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "#from torch.nn import Sigmoid\n",
        "#from torch.distributions import Bernoulli\n",
        "\n",
        "\n",
        "class BasicDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The BasicDecoder module takes an steganographic image and attempts to decode\n",
        "    the embedded data tensor.\n",
        "\n",
        "    Input: (N, 3, H, W)\n",
        "    Output: (N, D, H, W)\n",
        "    \"\"\"\n",
        "    def _name(self):\n",
        "      return \"BasicDecoder\"\n",
        "\n",
        "    def _conv2d(self, in_channels, out_channels):\n",
        "        return nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1\n",
        "        )\n",
        "\n",
        "    def _build_models(self):\n",
        "        self.conv1 = nn.Sequential(\n",
        "            self._conv2d(3, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size, self.data_depth),\n",
        "            #nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "        return self.conv1, self.conv2, self.conv3, self.conv4\n",
        "\n",
        "    def forward(self, image):\n",
        "        x = self._models[0](image)\n",
        "        x_1 = self._models[1](x)\n",
        "        x_2 = self._models[2](x_1)\n",
        "        x_3 = self._models[3](x_2)\n",
        "        #x_4 = Bernoulli(x_3).sample()\n",
        "        return x_3\n",
        "\n",
        "    def __init__(self, data_depth, hidden_size):\n",
        "        super().__init__()\n",
        "        self.data_depth = data_depth\n",
        "        self.hidden_size = hidden_size\n",
        "        self._models = self._build_models()\n",
        "        self.name = self._name()\n",
        "\n",
        "\n",
        "class DenseDecoder(BasicDecoder):\n",
        "    def _name(self):\n",
        "      return \"DenseDecoder\"\n",
        "\n",
        "    def _build_models(self):\n",
        "        self.conv1 = super()._build_models()[0]\n",
        "        self.conv2 = super()._build_models()[1]\n",
        "        self.conv3 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size * 2, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size)\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size * 3, self.data_depth),\n",
        "            #nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "        return self.conv1, self.conv2, self.conv3, self.conv4\n",
        "\n",
        "    def forward(self, image):\n",
        "        x = self._models[0](image)\n",
        "        x_list = [x]\n",
        "        x_1 = self._models[1](torch.cat(x_list, dim=1))\n",
        "        x_list.append(x_1)\n",
        "        x_2 = self._models[2](torch.cat(x_list, dim=1))\n",
        "        x_list.append(x_2)\n",
        "        x_3 = self._models[3](torch.cat(x_list, dim=1))\n",
        "        x_list.append(x_3)\n",
        "        return x_3\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he1SeqWRKtb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class BasicCritic(nn.Module):\n",
        "    \"\"\"\n",
        "    The BasicCritic module takes an image and predicts whether it is a cover\n",
        "    image or a steganographic image (N, 1).\n",
        "\n",
        "    Input: (N, 3, H, W)\n",
        "    Output: (N, 1)\n",
        "    \"\"\"\n",
        "    def _name(self):\n",
        "      return \"BasicCritic\"\n",
        "\n",
        "    def _conv2d(self, in_channels, out_channels):\n",
        "        return nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3\n",
        "        )\n",
        "\n",
        "    def _build_models(self):\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            self._conv2d(3, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size, self.hidden_size),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.BatchNorm2d(self.hidden_size),\n",
        "        )  \n",
        "        self.conv4 = nn.Sequential(\n",
        "            self._conv2d(self.hidden_size, 1)\n",
        "        )         \n",
        "\n",
        "        return self.conv1,self.conv2,self.conv3,self.conv4\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self._models = self._build_models()\n",
        "        self.name = self._name()\n",
        "\n",
        "    def forward(self, image):\n",
        "        x = self._models[0](image)\n",
        "        x_1 = self._models[1](x)\n",
        "        x_2 = self._models[2](x_1)\n",
        "        x_3 = self._models[3](x_2)\n",
        "        return torch.mean(x_3.view(x_3.size(0), -1), dim=1)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVLWLMetCrGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot(name, train_epoch, values, path, save):\n",
        "    clear_output(wait=True)\n",
        "    plt.close('all')\n",
        "    fig = plt.figure()\n",
        "    fig = plt.ion()\n",
        "    fig = plt.subplot(1, 1, 1)\n",
        "    fig = plt.title('epoch: %s -> %s: %s' % (train_epoch, name, values[-1]))\n",
        "    fig = plt.ylabel(name)\n",
        "    fig = plt.xlabel('validation_set')\n",
        "    fig = plt.plot(values)\n",
        "    fig = plt.grid()\n",
        "    get_fig = plt.gcf()\n",
        "    fig = plt.draw()  # draw the plot\n",
        "    fig = plt.pause(1)  # show it for 1 second\n",
        "    if save:\n",
        "        now = datetime.datetime.now()\n",
        "        get_fig.savefig('%s/%s_%.3f_%d_%s.png' %\n",
        "                        (path, name, train_epoch, values[-1], now.strftime(\"%Y-%m-%d_%H:%M:%S\")))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZCf5oX12gWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(encoder,decoder,data_depth,train_epoch,cover,payload):\n",
        "  %matplotlib inline\n",
        "  generated = encoder.forward(cover, payload)\n",
        "  decoded = decoder.forward(generated)\n",
        "  decoder_loss = binary_cross_entropy_with_logits(decoded, payload)\n",
        "  decoder_acc = (decoded >= 0.0).eq(\n",
        "    payload >= 0.5).sum().float() / payload.numel() # .numel() calculate the number of element in a tensor\n",
        "  print(\"Decoder loss: %.3f\"% decoder_loss.item())\n",
        "  print(\"Decoder acc: %.3f\"% decoder_acc.item())\n",
        "  f, ax = plt.subplots(1, 2)\n",
        "  plt.title(\"%s_%s\"%(encoder.name,decoder.name))\n",
        "  cover=np.transpose(np.squeeze(cover.cpu()), (1, 2, 0))\n",
        "  ax[0].imshow(cover)\n",
        "  ax[0].axis('off')\n",
        "  generated=np.transpose(np.squeeze((generated.cpu()).detach().numpy()), (1, 2, 0))\n",
        "  ax[1].imshow(generated)\n",
        "  ax[1].axis('off')\n",
        "  now = datetime.datetime.now()\n",
        "  print(\"payload :\")\n",
        "  print(payload)\n",
        "  print(\"decoded :\")\n",
        "  decoded[decoded<0]=0\n",
        "  decoded[decoded>0]=1\n",
        "  print(decoded)\n",
        "  now = datetime.datetime.now()\n",
        "\n",
        "  plt.savefig('results/samples/%s_%s_%d_%.3f_%d_%s.png' %\n",
        "              (encoder.name,decoder.name, data_depth,decoder_acc, train_epoch, now.strftime(\"%Y-%m-%d_%H:%M:%S\")))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHGbk-t3imrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(encoder,decoder,critic,en_de_optimizer,cr_optimizer,metrics,ep):\n",
        "    now = datetime.datetime.now()\n",
        "    cover_score = metrics['val.cover_score'][-1]\n",
        "    name = \"%s_%s_%+.3f_%s.dat\" % (encoder.name,decoder.name,cover_score,\n",
        "                                   now.strftime(\"%Y-%m-%d_%H:%M:%S\"))\n",
        "    fname = os.path.join('.', 'results/model', name)\n",
        "    states = {\n",
        "            'state_dict_critic': critic.state_dict(),\n",
        "            'state_dict_encoder': encoder.state_dict(),\n",
        "            'state_dict_decoder': decoder.state_dict(),\n",
        "            'en_de_optimizer': en_de_optimizer.state_dict(),\n",
        "            'cr_optimizer': cr_optimizer.state_dict(),\n",
        "            'metrics': metrics,\n",
        "            'train_epoch': ep,\n",
        "            'date': now.strftime(\"%Y-%m-%d_%H:%M:%S\"),\n",
        "    }\n",
        "    torch.save(states, fname)\n",
        "    path='results/plots/train_%s_%s_%s'% (encoder.name,decoder.name,now.strftime(\"%Y-%m-%d_%H:%M:%S\"))\n",
        "    try:\n",
        "      os.mkdir(os.path.join('.', path))\n",
        "    except Exception as error:\n",
        "      print(error)\n",
        "\n",
        "    plot('encoder_mse', ep, metrics['val.encoder_mse'], path, True)\n",
        "    plot('decoder_loss', ep, metrics['val.decoder_loss'], path, True)\n",
        "    plot('decoder_acc', ep, metrics['val.decoder_acc'], path, True)\n",
        "    plot('cover_score', ep, metrics['val.cover_score'], path, True)\n",
        "    plot('generated_score', ep, metrics['val.generated_score'], path, True)\n",
        "    plot('ssim', ep, metrics['val.ssim'], path, True)\n",
        "    plot('psnr', ep, metrics['val.psnr'], path, True)\n",
        "    plot('bpp', ep, metrics['val.bpp'], path, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMqZ9W6Ug6dR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_gan(encoder,decoder,critic,en_de_optimizer,cr_optimizer,metrics,train_loader,valid_loader):\n",
        "      for ep in range(epochs):\n",
        "        print(\"Epoch %d\" %(ep+1))\n",
        "        for cover, _ in notebook.tqdm(train_loader):\n",
        "            gc.collect()\n",
        "            cover = cover.to(device)\n",
        "            N, _, H, W = cover.size()\n",
        "            # sampled from the discrete uniform distribution over 0 to 2\n",
        "            payload = torch.zeros((N, data_depth, H, W),\n",
        "                                  device=device).random_(0, 2)\n",
        "            generated = encoder.forward(cover, payload)\n",
        "            cover_score = torch.mean(critic.forward(cover))\n",
        "            generated_score = torch.mean(critic.forward(generated))\n",
        "\n",
        "            cr_optimizer.zero_grad()\n",
        "            (cover_score - generated_score).backward(retain_graph=False)\n",
        "            cr_optimizer.step()\n",
        "\n",
        "            for p in critic.parameters():\n",
        "                p.data.clamp_(-0.1, 0.1)\n",
        "            metrics['train.cover_score'].append(cover_score.item())\n",
        "            metrics['train.generated_score'].append(generated_score.item())\n",
        "\n",
        "        for cover, _ in notebook.tqdm(train_loader):\n",
        "            gc.collect()\n",
        "            cover = cover.to(device)\n",
        "            N, _, H, W = cover.size()\n",
        "            # sampled from the discrete uniform distribution over 0 to 2\n",
        "            payload = torch.zeros((N, data_depth, H, W),\n",
        "                                  device=device).random_(0, 2)\n",
        "            generated = encoder.forward(cover, payload)\n",
        "            decoded = decoder.forward(generated)\n",
        "            encoder_mse = mse_loss(generated, cover)\n",
        "            decoder_loss = binary_cross_entropy_with_logits(decoded, payload)\n",
        "            decoder_acc = (decoded >= 0.0).eq(\n",
        "                payload >= 0.5).sum().float() / payload.numel()\n",
        "            generated_score = torch.mean(critic.forward(generated))\n",
        "\n",
        "            en_de_optimizer.zero_grad()\n",
        "            (100 * encoder_mse + decoder_loss +\n",
        "             generated_score).backward()  # Why 100?\n",
        "            en_de_optimizer.step()\n",
        "\n",
        "            metrics['train.encoder_mse'].append(encoder_mse.item())\n",
        "            metrics['train.decoder_loss'].append(decoder_loss.item())\n",
        "            metrics['train.decoder_acc'].append(decoder_acc.item())\n",
        "\n",
        "        for cover, _ in notebook.tqdm(valid_loader):\n",
        "            gc.collect()\n",
        "            cover = cover.to(device)\n",
        "            N, _, H, W = cover.size()\n",
        "            # sampled from the discrete uniform distribution over 0 to 2\n",
        "            payload = torch.zeros((N, data_depth, H, W),\n",
        "                                  device=device).random_(0, 2)\n",
        "            generated = encoder.forward(cover, payload)\n",
        "            decoded = decoder.forward(generated)\n",
        "\n",
        "            encoder_mse = mse_loss(generated, cover)\n",
        "            decoder_loss = binary_cross_entropy_with_logits(decoded, payload)\n",
        "            decoder_acc = (decoded >= 0.0).eq(\n",
        "                payload >= 0.5).sum().float() / payload.numel()\n",
        "            generated_score = torch.mean(critic.forward(generated))\n",
        "            cover_score = torch.mean(critic.forward(cover))\n",
        "\n",
        "            metrics['val.encoder_mse'].append(encoder_mse.item())\n",
        "            metrics['val.decoder_loss'].append(decoder_loss.item())\n",
        "            metrics['val.decoder_acc'].append(decoder_acc.item())\n",
        "            metrics['val.cover_score'].append(cover_score.item())\n",
        "            metrics['val.generated_score'].append(generated_score.item())\n",
        "            metrics['val.ssim'].append(\n",
        "                ssim(cover, generated).item())\n",
        "            metrics['val.psnr'].append(\n",
        "                10 * torch.log10(4 / encoder_mse).item())\n",
        "            metrics['val.bpp'].append(\n",
        "                data_depth * (2 * decoder_acc.item() - 1))\n",
        "        print('encoder_mse: %.3f - decoder_loss: %.3f - decoder_acc: %.3f - cover_score: %.3f - generated_score: %.3f - ssim: %.3f - psnr: %.3f - bpp: %.3f'\n",
        "          %(encoder_mse.item(),decoder_loss.item(),decoder_acc.item(),cover_score.item(),generated_score.item(), ssim(cover, generated).item(),10 * torch.log10(4 / encoder_mse).item(),data_depth * (2 * decoder_acc.item() - 1)))\n",
        "      save_model(encoder,decoder,critic,en_de_optimizer,cr_optimizer,metrics,ep)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnpEj9wjN-G-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fig2data ( fig ):\n",
        "    \"\"\"\n",
        "    @brief Convert a Matplotlib figure to a 4D numpy array with RGB channels and return it\n",
        "    @param fig a matplotlib figure\n",
        "    @return a numpy 3D array of RGB values\n",
        "    \"\"\"\n",
        "    # draw the renderer\n",
        "    fig.canvas.draw ( )\n",
        " \n",
        "    # Get the RGB buffer from the figure\n",
        "    w,h = fig.canvas.get_width_height()\n",
        "    buf = np.frombuffer ( fig.canvas.tostring_rgb(), dtype=np.uint8 )\n",
        "    buf.shape = ( w, h,3 )\n",
        " \n",
        "    return buf\n",
        "\n",
        "def has_file_allowed_extension(filename, extensions):\n",
        "    \"\"\"Checks if a file is an allowed extension.\n",
        "\n",
        "    Args:\n",
        "        filename (string): path to a file\n",
        "        extensions (tuple of strings): extensions to consider (lowercase)\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the filename ends with one of given extensions\n",
        "    \"\"\"\n",
        "    return filename.lower().endswith(extensions)\n",
        "\n",
        "\n",
        "def is_image_file(filename):\n",
        "    \"\"\"Checks if a file is an allowed image extension.\n",
        "\n",
        "    Args:\n",
        "        filename (string): path to a file\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the filename ends with a known image extension\n",
        "    \"\"\"\n",
        "    return has_file_allowed_extension(filename, AUD_EXTENSIONS)\n",
        "\n",
        "\n",
        "def make_dataset(dir, class_to_idx, extensions=None, is_valid_file=None):\n",
        "    images = []\n",
        "    dir = os.path.expanduser(dir)\n",
        "    if not ((extensions is None) ^ (is_valid_file is None)):\n",
        "        raise ValueError(\"Both extensions and is_valid_file cannot be None or not None at the same time\")\n",
        "    if extensions is not None:\n",
        "        def is_valid_file(x):\n",
        "            return has_file_allowed_extension(x, extensions)\n",
        "    for target in sorted(class_to_idx.keys()):\n",
        "        d = os.path.join(dir, target)\n",
        "        if not os.path.isdir(d):\n",
        "            continue\n",
        "        for root, _, fnames in sorted(os.walk(d, followlinks=True)):\n",
        "            for fname in sorted(fnames):\n",
        "                path = os.path.join(root, fname)\n",
        "                if is_valid_file(path):                    \n",
        "                    item = (path, class_to_idx[target])\n",
        "                    images.append(item)\n",
        "    return images\n",
        "\n",
        "\n",
        "def pil_loader(path):\n",
        "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "    with open(path, 'rb') as f:\n",
        "        y, sr = librosa.load(f.name) # your file\n",
        "        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
        "        img = librosa.display.specshow(librosa.power_to_db(S, ref=np.max), fmax=8000).get_figure()\n",
        "        f=fig2data(img)\n",
        "        w, h, d = f.shape\n",
        "        img=Image.frombytes( \"RGB\", ( w ,h ), f.tostring( ) )\n",
        "        return img.convert('RGB')\n",
        "\n",
        "\n",
        "def accimage_loader(path):\n",
        "    import accimage\n",
        "    try:\n",
        "        return accimage.Image(path)\n",
        "    except IOError:\n",
        "        # Potentially a decoding problem, fall back to PIL.Image\n",
        "        return pil_loader(path)\n",
        "\n",
        "\n",
        "def default_loader(path):\n",
        "    from torchvision import get_image_backend\n",
        "    if get_image_backend() == 'accimage':\n",
        "        return accimage_loader(path)\n",
        "    else:\n",
        "        return pil_loader(path)\n",
        "\n",
        "\n",
        "class AudToImageFolder(datasets.DatasetFolder):\n",
        "    \"\"\"A generic data loader where the images are arranged in this way: ::\n",
        "\n",
        "        root/dog/xxx.png\n",
        "        root/dog/xxy.png\n",
        "        root/dog/xxz.png\n",
        "\n",
        "        root/cat/123.png\n",
        "        root/cat/nsdf3.png\n",
        "        root/cat/asd932_.png\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory path.\n",
        "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "        loader (callable, optional): A function to load an image given its path.\n",
        "        is_valid_file (callable, optional): A function that takes path of an Image file\n",
        "            and check if the file is a valid file (used to check of corrupt files)\n",
        "\n",
        "     Attributes:\n",
        "        classes (list): List of the class names.\n",
        "        class_to_idx (dict): Dict with items (class_name, class_index).\n",
        "        imgs (list): List of (image path, class_index) tuples\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root, transform=None, target_transform=None,\n",
        "                 loader=default_loader, is_valid_file=None):\n",
        "        super(AudToImageFolder, self).__init__(root, loader, AUD_EXTENSIONS if is_valid_file is None else None,\n",
        "                                          transform=transform,\n",
        "                                          target_transform=target_transform,\n",
        "                                          is_valid_file=is_valid_file)\n",
        "        self.imgs = self.samples\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9fe79d2b-36d9-44f0-f076-b72be4603cac",
        "id": "cQLj2IDbTvOS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "6bea0c9de09640e3993372d36775e552",
            "67baca1aec9f452e8f3e4ff08896a4f4",
            "787da502c8df4c78bd7b804e700671f0",
            "4cb603612a52484cac4af3803ef18b46",
            "08fe95a8994a48cbb086f036f37961d1",
            "b3a441bc8a6b422085735ec55507f283",
            "82540accf51f4368adf429f87e8ab6e2",
            "89f6cf8573844db59a71bbf53a7befa6"
          ]
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  for func in [\n",
        "            lambda: os.mkdir(os.path.join('.', 'results')),\n",
        "            lambda: os.mkdir(os.path.join('.', 'results/model')),\n",
        "            lambda: os.mkdir(os.path.join('.', 'results/plots'))]:  # create directories\n",
        "    try:\n",
        "      func()\n",
        "    except Exception as error:\n",
        "      print(error)\n",
        "      continue\n",
        "\n",
        "  METRIC_FIELDS = [\n",
        "        'val.encoder_mse',\n",
        "        'val.decoder_loss',\n",
        "        'val.decoder_acc',\n",
        "        'val.cover_score',\n",
        "        'val.generated_score',\n",
        "        'val.ssim',\n",
        "        'val.psnr',\n",
        "        'val.bpp',\n",
        "        'train.encoder_mse',\n",
        "        'train.decoder_loss',\n",
        "        'train.decoder_acc',\n",
        "        'train.cover_score',\n",
        "        'train.generated_score',\n",
        "  ]\n",
        "\n",
        "  mu = [.5, .5, .5]\n",
        "  sigma = [.5, .5, .5]\n",
        "\n",
        "\n",
        "  transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                    transforms.RandomCrop(\n",
        "                                        360, pad_if_needed=True),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize(mu, sigma)])\n",
        "  if audio:\n",
        "    data_dir=\"LibriSpeech/devclean\" # directory to audio\n",
        "    train_set = AudToImageFolder(os.path.join(\n",
        "        data_dir, \"train/\"), transform=transform)\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_set, batch_size=4, shuffle=True)\n",
        "    valid_set = AudToImageFolder(os.path.join( \n",
        "        data_dir, \"val/\"), transform=transform)\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        valid_set, batch_size=4, shuffle=False)\n",
        "  else:\n",
        "    data_dir = 'div2k'\n",
        "    train_set = datasets.ImageFolder(os.path.join(\n",
        "        data_dir, \"train/\"), transform=transform)\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_set, batch_size=4, shuffle=True)\n",
        "    valid_set = datasets.ImageFolder(os.path.join( \n",
        "        data_dir, \"val/\"), transform=transform)\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        valid_set, batch_size=4, shuffle=False)\n",
        "\n",
        "  encoder = DenseEncoder(data_depth, hidden_size).to(device)\n",
        "  decoder = DenseDecoder(data_depth, hidden_size).to(device)\n",
        "  critic = BasicCritic(hidden_size).to(device)\n",
        "  cr_optimizer = Adam(critic.parameters(), lr=1e-4)\n",
        "  en_de_optimizer = Adam(list(decoder.parameters()) +\n",
        "                           list(encoder.parameters()), lr=1e-4)\n",
        "  metrics = {field: list() for field in METRIC_FIELDS}\n",
        "\n",
        "  if LOAD_MODEL: \n",
        "    if torch.cuda.is_available():\n",
        "      checkpoint = torch.load(PATH)\n",
        "    else:\n",
        "      checkpoint = torch.load(PATH, map_location=lambda storage, loc: storage)\n",
        "              \n",
        "    critic.load_state_dict(checkpoint['state_dict_critic'])\n",
        "    encoder.load_state_dict(checkpoint['state_dict_encoder'])\n",
        "    decoder.load_state_dict(checkpoint['state_dict_decoder'])\n",
        "    en_de_optimizer.load_state_dict(checkpoint['en_de_optimizer'])\n",
        "    cr_optimizer.load_state_dict(checkpoint['cr_optimizer'])\n",
        "    metrics=checkpoint['metrics']\n",
        "    ep=checkpoint['train_epoch']\n",
        "    date=checkpoint['date']\n",
        "    critic.train()\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    print('GAN loaded')\n",
        "    print(critic)\n",
        "    print(encoder)\n",
        "    print(decoder)\n",
        "    print(en_de_optimizer)\n",
        "    print(cr_optimizer)\n",
        "    print(date)\n",
        "  else:\n",
        "    fit_gan(encoder,decoder,critic,en_de_optimizer,cr_optimizer,metrics,train_loader,valid_loader)\n",
        "                "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 17] File exists: './results'\n",
            "[Errno 17] File exists: './results/model'\n",
            "[Errno 17] File exists: './results/plots'\n",
            "Epoch 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6bea0c9de09640e3993372d36775e552",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=223), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFFfYzJVK-I6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cover, _ = next(iter(valid_set))\n",
        "_, H, W = cover.size()\n",
        "cover = cover[None].to(device)\n",
        "payload = torch.zeros((1, data_depth, H, W),device=device).random_(0, 2)\n",
        "test(encoder,decoder,data_depth,epochs,cover,payload)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjwtoNP4ed9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}